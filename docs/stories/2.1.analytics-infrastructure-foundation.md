# Story 2.1: Analytics Infrastructure Foundation

## Status
Draft

## Story
**As a** system administrator,  
**I want** a robust analytics infrastructure that captures query performance data asynchronously,  
**so that** I can monitor KB health and user satisfaction without impacting MCP server performance.

## Acceptance Criteria
1. **Async Analytics Service**: Create analytics service with SQLite database that processes data collection without blocking MCP operations
2. **Core Data Models**: Implement QueryAnalytics, DocumentAnalytics, and SystemMetrics data models with comprehensive schema
3. **KPI Calculation Framework**: Build calculation methods for primary KPIs (satisfaction rate, success rate, response time, diversity index)
4. **Container Integration**: Register analytics service in existing dependency injection container following established patterns
5. **Performance Isolation**: Analytics processing must not add measurable latency to existing MCP tool operations

## Tasks / Subtasks
- [ ] Create analytics service infrastructure (AC: 1, 5)
  - [ ] Create `mcp_rag_playground/analytics/__init__.py` package
  - [ ] Create `AnalyticsService` class in `mcp_rag_playground/analytics/analytics_service.py`
  - [ ] Implement async queue processing with `asyncio.Queue` for non-blocking data collection
  - [ ] Add background processor method `process_analytics_queue()` for continuous data processing
  - [ ] Create database initialization and schema setup methods
- [ ] Design and implement data models (AC: 2)
  - [ ] Create `mcp_rag_playground/analytics/models.py` with comprehensive data classes
  - [ ] Implement `QueryAnalytics` model with all required fields (query_id, query_text, timestamp, result_count, response_time, similarity_scores, source_documents, user_rating, user_feedback, error_message)
  - [ ] Implement `DocumentAnalytics` model for document usage tracking
  - [ ] Implement `SystemMetrics` model for performance monitoring
  - [ ] Add validation and type checking for all data models
- [ ] Create SQLite database schema and operations (AC: 1, 2)
  - [ ] Design normalized SQLite schema for analytics data storage
  - [ ] Create database initialization script with tables: `query_analytics`, `document_analytics`, `system_metrics`, `user_feedback`
  - [ ] Implement database connection management with proper error handling
  - [ ] Add database migration utilities for schema evolution
  - [ ] Create indexes for efficient query performance on timestamp and rating fields
- [ ] Implement KPI calculation framework (AC: 3)
  - [ ] Create `calculate_kpis()` method with configurable time windows
  - [ ] Implement User Satisfaction Rate calculation (percentage of 4+ star ratings)
  - [ ] Implement Query Success Rate calculation (queries with results and similarity > 0.3)
  - [ ] Implement Average Response Time calculation with percentile distributions
  - [ ] Implement Source Diversity Index calculation (unique documents / total available)
  - [ ] Add KPI caching and performance optimization for real-time dashboard needs
- [ ] Container DI integration (AC: 4)
  - [ ] Update `mcp_rag_playground/container/container.py` to register `AnalyticsService`
  - [ ] Add analytics configuration settings to container configuration
  - [ ] Create `AnalyticsConfig` class in `mcp_rag_playground/config/analytics_config.py`
  - [ ] Ensure analytics service follows existing singleton pattern in container
- [ ] Performance testing and validation (AC: 5)
  - [ ] Create benchmark tests measuring MCP tool latency with and without analytics
  - [ ] Implement performance monitoring for analytics queue processing
  - [ ] Add memory usage tracking for analytics data structures
  - [ ] Validate that analytics processing has <1ms impact on query response time
- [ ] Unit testing for analytics infrastructure (AC: 1-5)
  - [ ] Test `AnalyticsService` async queue processing and data storage
  - [ ] Test data model validation and serialization
  - [ ] Test SQLite database operations and schema migrations
  - [ ] Test KPI calculation accuracy with sample data sets
  - [ ] Test container integration and dependency injection
  - [ ] Test performance isolation and non-blocking behavior

## Dev Notes

### Previous Story Insights
This is the foundation story for Epic 2, building upon Epic 1's enhanced document metadata tracking to provide analytics capabilities. The analytics infrastructure must integrate seamlessly with existing MCP tools and container DI system without affecting current performance characteristics.

### Data Models

**Core Analytics Models** (New Implementation Required):
```python
@dataclass
class QueryAnalytics:
    query_id: str
    query_text: str
    timestamp: datetime
    result_count: int
    response_time: float
    avg_similarity_score: float
    max_similarity_score: float
    source_documents: List[str]
    chunk_positions: List[int]
    user_rating: Optional[int] = None
    user_feedback: Optional[str] = None
    error_message: Optional[str] = None

@dataclass
class DocumentAnalytics:
    document_id: str
    query_id: str
    relevance_score: float
    chunk_position: int
    timestamp: datetime

@dataclass
class SystemMetrics:
    timestamp: datetime
    metric_name: str
    metric_value: float

@dataclass
class AnalyticsConfig:
    enabled: bool = True
    db_path: str = "analytics.db"
    collection_interval: int = 30
    retention_days: int = 90
    alert_thresholds: dict = field(default_factory=lambda: {
        'satisfaction_rate': 75,
        'success_rate': 85,
        'response_time': 3.0,
        'diversity_index': 0.5
    })
```

### Database Schema Specifications

**SQLite Schema Design** (Required Implementation):
```sql
-- Primary analytics table
CREATE TABLE query_analytics (
    query_id TEXT PRIMARY KEY,
    query_text TEXT NOT NULL,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    result_count INTEGER,
    response_time REAL,
    avg_similarity_score REAL,
    max_similarity_score REAL,
    source_document_count INTEGER,
    user_rating INTEGER CHECK (user_rating BETWEEN 1 AND 5),
    user_feedback TEXT,
    error_message TEXT,
    INDEX idx_timestamp (timestamp),
    INDEX idx_rating (user_rating),
    INDEX idx_response_time (response_time)
);

-- Document usage tracking
CREATE TABLE document_analytics (
    document_id TEXT,
    query_id TEXT,
    relevance_score REAL,
    chunk_position INTEGER,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (query_id) REFERENCES query_analytics(query_id),
    INDEX idx_document_id (document_id),
    INDEX idx_timestamp (timestamp)
);

-- System performance metrics
CREATE TABLE system_metrics (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
    metric_name TEXT,
    metric_value REAL,
    INDEX idx_timestamp_metric (timestamp, metric_name)
);
```

### Component Specifications

**Analytics Service Architecture** [Required New Component]:
The `AnalyticsService` must implement async queue-based data collection to ensure zero performance impact on MCP operations. Key requirements:
- Async queue processing using `asyncio.Queue` for non-blocking data ingestion
- Background task processor for continuous data storage to SQLite
- Configurable retention policies and data cleanup
- Thread-safe operations for concurrent access from MCP tools

**Container Integration Pattern** [Source: mcp_rag_playground/container/container.py]:
Analytics service must integrate with existing Container DI system following established singleton pattern:
```python
analytics_service = providers.Singleton(
    AnalyticsService,
    config=providers.Configuration.analytics
)
```

**Performance Requirements** [Source: Epic 2 Technical Requirements]:
- Analytics data collection must add <1ms latency to MCP tool operations
- Background processing should not exceed 5% CPU usage during normal operations
- Memory usage for analytics queue should not exceed 50MB under typical load
- SQLite operations must be optimized for write-heavy workloads

### File Locations

**Files to Create**:
- `mcp_rag_playground/analytics/__init__.py` - Analytics package initialization
- `mcp_rag_playground/analytics/analytics_service.py` - Core analytics service implementation
- `mcp_rag_playground/analytics/models.py` - Data models and schema definitions
- `mcp_rag_playground/config/analytics_config.py` - Analytics configuration management

**Files to Modify**:
- `mcp_rag_playground/container/container.py` - Register analytics service in DI container
- `mcp_rag_playground/__init__.py` - Export analytics components if needed

**Test Files to Create**:
- `mcp_rag_playground/tests/test_analytics_service.py` - Analytics service testing
- `mcp_rag_playground/tests/test_analytics_models.py` - Data model validation testing
- `mcp_rag_playground/tests/fixtures/analytics_fixtures.py` - Test data fixtures

### Testing Requirements

**Current Test Structure** [Source: docs/architecture/testing-reality.md]:
- Framework: pytest with markers system (unit, integration, slow, milvus, embedding, performance, smoke)
- Coverage target: 80% minimum
- Test files use `test_*.py` naming convention
- Fixtures organized in `fixtures/` directory

**Test Requirements for This Story**:
- Unit tests for analytics service components (marker: unit)
- Integration tests with SQLite database operations (marker: integration)
- Performance tests for async queue processing (marker: performance)
- Container DI integration tests (marker: unit, integration)
- Database schema and migration tests

### Technical Constraints

**Performance Requirements** [Source: Epic 2 KPIs]:
- Analytics must not impact existing MCP tool response times
- Background processing should be resource-efficient
- Database operations must be optimized for concurrent access
- Memory usage must be bounded and configurable

**Integration Verification Requirements**:
- All existing MCP functionality continues working unchanged
- Container DI patterns are followed consistently
- Database schema supports future analytics requirements
- Configuration system allows enabling/disabling analytics features

### Architecture Integration

**SOLID Principles Compliance**:
- Single Responsibility: Analytics service focuses solely on data collection and KPI calculation
- Open/Closed: Extensible for new metrics without modifying core functionality
- Dependency Inversion: Depends on configuration abstractions, not concrete implementations

**Existing Integration Points**:
- **Container DI**: Analytics service registered as singleton following existing patterns
- **Configuration System**: Leverages existing pydantic-based configuration approach
- **Testing Framework**: Uses established pytest infrastructure and marker system
- **Performance Standards**: Maintains existing response time characteristics

## Testing

**Test File Location**: `mcp_rag_playground/tests/` following existing pattern

**Test Standards**:
- Use pytest framework with existing fixtures from `conftest.py`
- Follow existing marker system (unit, integration, performance)
- Achieve 80% minimum code coverage for analytics components
- Use SQLite in-memory database for unit tests
- Mock external dependencies for isolated testing

**Testing Frameworks and Patterns**:
- Pytest with comprehensive markers system
- SQLite testing with temporary databases
- Async testing with pytest-asyncio
- Performance benchmarking for queue processing
- Container integration testing with dependency injection

**Specific Testing Requirements**:
- Analytics service async queue processing: Test queue ingestion and background processing
- Data model validation: Test all data classes with valid and invalid inputs
- Database operations: Test CRUD operations, schema migrations, and indexing
- KPI calculation accuracy: Test mathematical correctness with known data sets
- Performance isolation: Benchmark MCP tool response times with analytics enabled/disabled
- Container integration: Test dependency injection and service lifecycle management

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|-----------|
| 2025-08-16 | 1.0 | Initial story creation for analytics infrastructure foundation | PM John |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
*This section will be populated by the QA agent after story completion*