# Story 1.4: Smart Document Processing Pipeline

## Status
Approved

## Story
**As a** knowledge base administrator,  
**I want** intelligent document processing capabilities,  
**so that** I can maintain high-quality knowledge base content with minimal manual intervention.

## Acceptance Criteria
1. **Document Categorization**: Automatically categorize documents based on filename and file extension patterns (e.g., .py files = "code", .md files = "documentation", README* = "documentation")
2. **Duplicate Detection**: Check for existing documents with the same filename before ingestion, with configurable action (skip, overwrite, or version)
3. **Format Conversion**: Extract text content from PDF and DOCX files to extend support beyond current 15+ file types
4. **Processing Pipeline**: Execute categorization, duplicate detection, and format conversion as automated pre-processing steps before existing chunking and embedding workflows

## Tasks / Subtasks
- [ ] Create smart processing pipeline framework (AC: 1, 4)
  - [ ] Create DocumentProcessingPipeline class in `mcp_rag_playground/vectordb/processor/smart_processor.py`
  - [ ] Implement ProcessingStep abstract interface for modular pipeline components
  - [ ] Integrate with existing DocumentProcessor (800-char chunking, 200-char overlap)
  - [ ] Add configuration support for enabling/disabling smart processing features
- [ ] Implement filename-based document categorization (AC: 1)
  - [ ] Create DocumentCategorizer class with filename pattern matching rules
  - [ ] Define category mapping rules (e.g., .py/.js/.ts = "code", .md/.txt/README* = "documentation", .json/.yaml/.xml = "config")
  - [ ] Add category field to Document metadata using patterns from filename and extension
  - [ ] Support configurable custom category rules for specific projects
- [ ] Implement filename-based duplicate detection (AC: 2)
  - [ ] Create DuplicateDetector class that checks for existing documents with same filename
  - [ ] Query existing documents in knowledge base by filename before ingestion
  - [ ] Support configurable duplicate handling actions: skip, overwrite, or create versioned filename
  - [ ] Add pre-ingestion duplicate checking in VectorClient.upload method with user-friendly messaging
- [ ] Implement PDF and DOCX text extraction (AC: 3)
  - [ ] Create FormatConverter class that extracts readable text from binary file formats
  - [ ] Add PDF text extraction using PyPDF2 or similar library (converts PDF → plain text)
  - [ ] Add DOCX text extraction using python-docx library (converts DOCX → plain text)
  - [ ] Integrate converters with existing DocumentProcessor so converted files are processed like .txt files
- [ ] Update RagAPI and MCP tools (AC: 4)
  - [ ] Modify RagAPI.add_document to use smart processing pipeline
  - [ ] Add smart processing configuration options to MCP tools
  - [ ] Update MCP add_document_from_file tool with smart processing options
  - [ ] Maintain backward compatibility for existing simple ingestion workflows
- [ ] Unit testing for smart processing pipeline (AC: 1-4)
  - [ ] Test filename-based categorization with various file types and patterns
  - [ ] Test filename duplicate detection with existing and new documents
  - [ ] Test PDF and DOCX text extraction with sample files
  - [ ] Test processing pipeline sequence: duplicates → conversion → categorization → chunking
  - [ ] Test configuration options for enabling/disabling each processing step

## Dev Notes

### Previous Story Insights
From Story 1.3, the system now has complete document management operations including enhanced add_document functionality, document removal, and batch operations. The enhanced Document model with 8 metadata fields is fully implemented and integrated. The system maintains 15+ file type support through the processor pipeline in `mcp_rag_playground/vectordb/processor/`. All operations use Container DI patterns and integrate with the existing metadata tracking foundation from Story 1.1.

### Data Models

**Enhanced Document Model** [Source: Story 1.1 Implementation]:
```python
@dataclass
class Document:
    content: str
    metadata: Dict[str, Any]
    id: Optional[str] = None
    # Enhanced metadata fields from Story 1.1
    filename: Optional[str] = None
    file_type: Optional[str] = None
    ingestion_timestamp: Optional[datetime] = None
    chunk_count: Optional[int] = None
    file_size: Optional[int] = None
    chunk_position: Optional[int] = None
    vector_id: Optional[str] = None
    embedding_status: str = "pending"
    # New field for Story 1.4
    category: Optional[str] = None
```

**Current DocumentProcessor Class** [Source: docs/architecture/source-tree-and-module-organization.md#25-26]:
```python
class DocumentProcessor:
    """Handles document processing for vector database ingestion."""
    
    def __init__(self, chunk_size: int = 800, overlap: int = 200, 
                 custom_processors: Optional[Dict[str, FileProcessor]] = None):
        # Supports 15+ file types: .txt, .md, .py, .json, .js, .ts, .css, .html, .yml, .yaml, .log
```

**Required New Classes**:
- `DocumentProcessingPipeline` - Main orchestrator that executes processing steps in sequence
- `ProcessingStep` - Abstract interface for pipeline components
- `DocumentCategorizer` - Filename/extension-based categorization using simple pattern matching
- `DuplicateDetector` - Filename-based duplicate detection with configurable actions
- `FormatConverter` - PDF/DOCX text extraction for unsupported file formats

### API Specifications

**Filename-Based Processing Approach**:
The simplified smart processing uses filename patterns and basic text extraction instead of complex embedding calculations. This makes the system faster, more predictable, and easier to configure.

**Current VectorClient.upload Method** [Source: Story 1.1/1.3 Context]:
```python
def upload(self, file_path: str, metadata: Optional[Dict[str, Any]] = None) -> bool:
    # Current implementation processes single file with existing DocumentProcessor
    # Returns boolean success/failure
```

**Required VectorClient Extensions**:
- Add smart processing configuration parameters to upload method
- Integrate DocumentProcessingPipeline before chunking and embedding
- Maintain backward compatibility with existing simple workflows

**Current MCP Tools** [Source: docs/architecture/data-models-and-apis.md#12-16]:
```python
@mcp.tool()
def add_document_from_file(ctx: Context, file_path: str) -> Dict[str, Any]:
    """Add a document to the knowledge base from a file with intelligent processing."""
    # Current tool supports single file with path normalization
```

**Required MCP Tool Extensions**:
- Add optional smart processing configuration parameters
- Add enable_smart_processing: bool = True parameter
- Add enable_categorization: bool = True parameter  
- Add enable_duplicate_detection: bool = True parameter
- Add duplicate_action: str = "skip" parameter (options: "skip", "overwrite", "version")

### Component Specifications

**Current Container DI Integration** [Source: docs/architecture/extension-patterns-and-architectural-conventions.md#16]:
The Container system follows dependency inversion patterns with all components depending on abstractions. Smart processing components must integrate through the existing DI container using the same patterns established for VectorClient, RagAPI, and embedding services.

**File Type Support Extension** [Source: docs/architecture/source-tree-and-module-organization.md#25-26]:
Current system supports 15+ file types through `vectordb/processor/file_processor.py`. Smart processing must extend this to include PDF and DOCX extraction while maintaining existing processors for .txt, .md, .py, .json, .js, .ts, .css, .html, .yml, .yaml, .log files.

**Simplified Processing Approach**:
Smart processing uses filename patterns and basic file operations instead of embedding calculations. This approach is faster, more reliable, and easier to configure than ML-based approaches while still providing valuable document organization features.

### File Locations

**Files to Create**:
- `mcp_rag_playground/vectordb/processor/smart_processor.py` - Smart processing pipeline and components
- `mcp_rag_playground/vectordb/processor/categorizer.py` - Document categorization logic
- `mcp_rag_playground/vectordb/processor/duplicate_detector.py` - Duplicate detection implementation
- `mcp_rag_playground/vectordb/processor/format_converter.py` - PDF/DOCX conversion utilities

**Files to Modify**:
- `mcp_rag_playground/vectordb/vector_client.py` - Integrate smart processing pipeline
- `mcp_rag_playground/rag/rag_api.py` - Add smart processing configuration options
- `mcp_rag_playground/mcp/rag_server.py` - Update MCP tools with smart processing parameters
- `mcp_rag_playground/container/container.py` - Register smart processing services in DI container

**Test Files to Create**:
- `mcp_rag_playground/tests/test_smart_processing.py` - Smart processing pipeline tests
- `mcp_rag_playground/tests/test_document_categorization.py` - Categorization accuracy tests
- `mcp_rag_playground/tests/test_duplicate_detection.py` - Duplicate detection tests
- `mcp_rag_playground/tests/test_format_conversion.py` - PDF/DOCX conversion tests

### Testing Requirements

**Current Test Structure** [Source: docs/architecture/testing-reality.md#5-22]:
- Framework: pytest with markers system
- Coverage target: 80% minimum  
- Markers: unit, integration, slow, milvus, embedding, performance, smoke
- Test files use `test_*.py` naming convention
- Fixtures organized by concern in `fixtures/` directory

**Test Requirements for This Story**:
- Unit tests for smart processing components (marker: unit)
- Integration tests with real embedding service (marker: embedding, integration)
- Performance tests for duplicate detection accuracy (marker: performance)
- Tests with sample PDF/DOCX files (marker: slow, integration)
- MCP tool tests with smart processing options

### Technical Constraints

**Performance Requirements** [Source: Epic Story 1.4]:
- IV2: New processing steps don't significantly impact ingestion performance (<20% overhead)
- Smart processing must be optional to maintain existing simple workflows  
- Filename-based operations must be fast enough for real-time use (no complex calculations)

**Integration Verification Requirements** [Source: Epic Story 1.4]:
- IV1: Smart processing features can be disabled to maintain existing simple ingestion workflows
- IV3: Processing results integrate seamlessly with existing metadata tracking from Story 1.1

**Simplified Processing Benefits**:
- Faster than embedding-based approaches (no ML calculations required)
- More predictable results (rule-based logic instead of similarity thresholds)
- Easier to configure and debug (simple filename patterns)

### Architecture Integration

**SOLID Principles Compliance** [Source: docs/architecture/extension-patterns-and-architectural-conventions.md#3-16]:
- Single Responsibility: Each processing component handles one specific task (categorization, duplicate detection, format conversion)
- Open/Closed: New processing steps extend existing interfaces without modification
- Dependency Inversion: All components depend on abstractions (ProcessingStep interface, not embedding services)

**Processing Pipeline Pattern**:
```python
class DocumentProcessingPipeline:
    def __init__(self, steps: List[ProcessingStep], enabled: bool = True):
        self.steps = steps
        self.enabled = enabled
    
    def process(self, document: Document) -> Document:
        if not self.enabled:
            return document
        
        for step in self.steps:
            document = step.process(document)
        return document
```

**Container Integration Pattern** [Source: docs/architecture/extension-patterns-and-architectural-conventions.md#16]:
Smart processing services must be registered in the DI container following existing patterns:
```python
# In container.py
@singleton
def document_processing_pipeline(self) -> DocumentProcessingPipeline:
    return DocumentProcessingPipeline([
        self.document_categorizer(),
        self.duplicate_detector(),
        self.format_converter()
    ])
```

## Testing

**Test File Location**: `mcp_rag_playground/tests/` following existing pattern

**Test Standards**:
- Use pytest framework with existing fixtures from `conftest.py`
- Follow existing marker system (unit, integration, embedding, etc.)
- Achieve 80% minimum code coverage
- Use real embedding service for integration tests
- Mock external dependencies for unit tests

**Testing Frameworks and Patterns**:
- Pytest with comprehensive markers system
- Real component integration tests (not just mocks)
- Coverage reporting with pytest-cov
- Fixtures organized by concern in `fixtures/` directory

**Specific Testing Requirements**:
- Filename-based categorization: Test pattern matching with various file types and naming conventions  
- Filename duplicate detection: Test with existing/new documents with same filenames
- PDF/DOCX text extraction: Test conversion of sample binary files to readable text
- Processing pipeline sequence: Test duplicates → conversion → categorization → chunking workflow
- Configuration testing: Verify each processing step can be enabled/disabled independently
- Backward compatibility: Ensure existing simple workflows continue unchanged

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-08-15 | 1.0 | Initial story creation | Scrum Master |
| 2025-08-15 | 1.1 | Simplified approach: filename-based categorization and duplicate detection, clearer format conversion definitions | Scrum Master |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
*This section will be populated by the QA agent after story completion*