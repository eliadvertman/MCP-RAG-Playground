# Story 2.2: MCP Tool Analytics Integration

## Status
Draft

## Story
**As a** knowledge base user,  
**I want** to provide feedback on query responses and have my interactions tracked for quality improvement,  
**so that** the system can learn and improve query quality over time without affecting my experience.

## Acceptance Criteria
1. **Enhanced Search Tool**: Extend existing `search_knowledge_base` MCP tool with non-blocking analytics data collection
2. **User Feedback System**: Create new `rate_query_response` MCP tool for 5-star rating with optional text feedback
3. **Async Data Collection**: All analytics data logging must be asynchronous with no measurable impact on MCP tool response times
4. **Query ID Tracking**: Implement query ID system for linking responses to feedback ratings
5. **Backward Compatibility**: Existing MCP tool functionality remains unchanged for current users
6. **Error Handling**: Analytics failures must not prevent normal MCP tool operation

## Tasks / Subtasks
- [ ] Enhance existing search MCP tool with analytics (AC: 1, 3, 5)
  - [ ] Modify `search_knowledge_base` tool in `mcp_rag_playground/mcp/rag_server.py`
  - [ ] Add unique query ID generation using `uuid.uuid4()` for each search request
  - [ ] Implement analytics data collection: response time, result count, similarity scores, source documents
  - [ ] Add async analytics logging using `asyncio.create_task()` for non-blocking operation
  - [ ] Ensure existing search functionality and response format remains unchanged
  - [ ] Add query_id to tool response for user feedback linking
- [ ] Create user feedback rating MCP tool (AC: 2, 4)
  - [ ] Create new `rate_query_response` MCP tool function
  - [ ] Implement 5-star rating validation (1-5 integer values)
  - [ ] Add optional text feedback parameter with length validation
  - [ ] Link ratings to query IDs for response tracking
  - [ ] Return confirmation response with rating acknowledgment
  - [ ] Add input validation and error handling for rating parameters
- [ ] Implement async analytics data collection framework (AC: 3, 6)
  - [ ] Create analytics data collection helper functions
  - [ ] Implement error handling that prevents analytics failures from affecting MCP tools
  - [ ] Add performance monitoring for analytics collection overhead
  - [ ] Create analytics data validation before queue submission
  - [ ] Implement fallback behavior when analytics service is unavailable
- [ ] Add comprehensive error handling and resilience (AC: 6)
  - [ ] Wrap analytics collection in try-catch blocks to prevent MCP tool failures
  - [ ] Add logging for analytics collection errors without exposing to users
  - [ ] Implement graceful degradation when analytics database is unavailable
  - [ ] Add configuration option to disable analytics collection entirely
  - [ ] Create health check for analytics service availability
- [ ] Update container integration for analytics access (AC: 1, 2)
  - [ ] Ensure MCP tools can access analytics service through existing container DI
  - [ ] Add analytics service dependency injection to MCP server module
  - [ ] Verify analytics service initialization before MCP tool startup
  - [ ] Add configuration-based analytics enablement/disablement
- [ ] Performance validation and testing (AC: 3, 5)
  - [ ] Benchmark MCP tool response times with analytics enabled vs disabled
  - [ ] Validate that analytics collection adds <1ms to search operations
  - [ ] Test MCP tool behavior when analytics service is unavailable
  - [ ] Verify backward compatibility with existing MCP tool clients
- [ ] Unit testing for enhanced MCP tools (AC: 1-6)
  - [ ] Test enhanced search tool with analytics collection enabled/disabled
  - [ ] Test user feedback rating tool with valid and invalid inputs
  - [ ] Test async analytics collection and error handling
  - [ ] Test query ID generation and linking between search and feedback
  - [ ] Test backward compatibility of existing MCP tool functionality
  - [ ] Test performance isolation and non-blocking analytics behavior

## Dev Notes

### Previous Story Insights
This story builds upon Story 2.1 (Analytics Infrastructure Foundation) to integrate analytics collection into existing MCP tools. The analytics service from Story 2.1 provides the async queue processing and data storage infrastructure that this story will use for non-blocking data collection.

### API Specifications

**Current MCP Tools** [Source: mcp_rag_playground/mcp/rag_server.py]:
```python
@mcp.tool()
def search_knowledge_base(ctx: Context, query: str, limit: int = 10, min_score: float = 0.7) -> Dict[str, Any]:
    """Search for documents in the knowledge base using semantic similarity."""
    # Current implementation returns search results without analytics
```

**Enhanced Search Tool** (Required Implementation):
```python
@mcp.tool()
def search_knowledge_base(ctx: Context, query: str, limit: int = 10, min_score: float = 0.7) -> Dict[str, Any]:
    """Enhanced search with analytics tracking"""
    query_id = str(uuid.uuid4())
    start_time = time.time()
    
    try:
        # Existing search logic (unchanged)
        results = rag_api.search(query, limit, min_score)
        response_time = time.time() - start_time
        
        # Async analytics collection (new)
        analytics_data = QueryAnalytics(
            query_id=query_id,
            query_text=query,
            timestamp=datetime.now(),
            result_count=len(results),
            response_time=response_time,
            avg_similarity_score=sum(r.score for r in results) / len(results) if results else 0,
            max_similarity_score=max(r.score for r in results) if results else 0,
            source_documents=[r.document.filename for r in results],
            chunk_positions=[r.document.chunk_position for r in results]
        )
        
        # Non-blocking analytics logging
        asyncio.create_task(analytics_service.log_query_async(analytics_data))
        
        # Return results with query_id for feedback linking
        return {
            "query_id": query_id,  # New field for feedback linking
            "results": [{"document": r.document.dict(), "score": r.score} for r in results],
            "metadata": {
                "result_count": len(results),
                "response_time": response_time,
                "query_processed_at": datetime.now().isoformat()
            }
        }
        
    except Exception as e:
        # Error analytics collection
        error_analytics = QueryAnalytics(
            query_id=query_id,
            query_text=query,
            timestamp=datetime.now(),
            result_count=0,
            response_time=time.time() - start_time,
            avg_similarity_score=0,
            max_similarity_score=0,
            source_documents=[],
            chunk_positions=[],
            error_message=str(e)
        )
        asyncio.create_task(analytics_service.log_query_async(error_analytics))
        raise
```

**New User Feedback Tool** (Required Implementation):
```python
@mcp.tool()
def rate_query_response(ctx: Context, query_id: str, rating: int, feedback: str = "") -> Dict[str, Any]:
    """Allow users to rate query response quality"""
    if not 1 <= rating <= 5:
        raise ValueError("Rating must be between 1 and 5")
        
    if len(feedback) > 1000:
        raise ValueError("Feedback text must be 1000 characters or less")
        
    try:
        # Async feedback logging
        asyncio.create_task(analytics_service.log_user_feedback(query_id, rating, feedback))
        
        return {
            "status": "success",
            "message": f"Feedback recorded for query {query_id}",
            "rating": rating,
            "feedback_received": bool(feedback)
        }
    except Exception as e:
        # Log error but don't expose to user
        logger.error(f"Failed to record feedback for query {query_id}: {e}")
        return {
            "status": "error",
            "message": "Failed to record feedback. Please try again.",
            "query_id": query_id
        }
```

### Component Specifications

**Analytics Integration Pattern** [Source: Story 2.1 Analytics Service]:
MCP tools must access the analytics service through the existing container DI system:
```python
# Analytics service access in MCP module
from mcp_rag_playground.container.container import Container

container = Container()
analytics_service = container.analytics_service()
```

**Non-blocking Data Collection** [Required Implementation]:
All analytics data collection must use async patterns to prevent blocking MCP tool operations:
- Use `asyncio.create_task()` for fire-and-forget analytics logging
- Implement comprehensive error handling to prevent analytics failures from affecting tool functionality
- Add performance monitoring to validate <1ms overhead requirement

**Query ID Linking System** [New Implementation]:
Implement UUID-based query tracking to link search results with user feedback:
- Generate unique query_id for each search request
- Include query_id in search response for user reference
- Link feedback ratings to original query using query_id
- Validate query_id exists before accepting feedback

### File Locations

**Files to Modify**:
- `mcp_rag_playground/mcp/rag_server.py` - Enhance search tool and add feedback tool
- `mcp_rag_playground/mcp/__init__.py` - Update imports if needed

**Files to Create**:
- `mcp_rag_playground/mcp/analytics_helpers.py` - Analytics collection helper functions (optional)

**Test Files to Create**:
- `mcp_rag_playground/tests/test_mcp_analytics_integration.py` - MCP tool analytics testing
- `mcp_rag_playground/tests/test_user_feedback.py` - User feedback system testing

### Testing Requirements

**Current Test Structure** [Source: docs/architecture/testing-reality.md]:
- Framework: pytest with markers system
- Coverage target: 80% minimum
- Test files use `test_*.py` naming convention
- MCP testing requires special considerations for async tool behavior

**Test Requirements for This Story**:
- Unit tests for enhanced MCP tools (marker: unit)
- Integration tests with analytics service (marker: integration)
- Performance tests for analytics overhead (marker: performance)
- MCP tool async behavior testing
- Backward compatibility verification tests

**MCP Tool Testing Pattern** [Required Implementation]:
```python
import pytest
from mcp_rag_playground.mcp.rag_server import search_knowledge_base, rate_query_response

@pytest.mark.asyncio
async def test_search_with_analytics():
    """Test enhanced search tool with analytics collection"""
    # Test search functionality unchanged
    result = search_knowledge_base(ctx, "test query", limit=5)
    
    assert "query_id" in result
    assert "results" in result
    assert "metadata" in result
    assert len(result["results"]) <= 5
    
    # Verify analytics data collection (async)
    await asyncio.sleep(0.1)  # Allow async processing
    # Verify analytics data was stored correctly

def test_rate_query_response():
    """Test user feedback rating tool"""
    query_id = "test-query-id"
    
    # Test valid rating
    result = rate_query_response(ctx, query_id, 5, "Great results!")
    assert result["status"] == "success"
    assert result["rating"] == 5
    
    # Test invalid rating
    with pytest.raises(ValueError):
        rate_query_response(ctx, query_id, 6, "Invalid rating")
```

### Technical Constraints

**Performance Requirements** [Source: Epic 2 AC 5]:
- Analytics collection must add <1ms latency to MCP tool operations
- All analytics processing must be asynchronous and non-blocking
- MCP tools must function normally even when analytics service is unavailable

**Backward Compatibility Requirements** [Source: Epic 2 AC 5]:
- Existing MCP tool clients must continue working without modification
- Tool response format must remain compatible (only additive changes)
- Tool behavior must be identical except for analytics collection

**Error Handling Requirements** [Source: AC 6]:
- Analytics failures must never prevent normal MCP tool operation
- Error logging should not expose technical details to end users
- Graceful degradation when analytics service is unavailable

### Architecture Integration

**SOLID Principles Compliance**:
- Single Responsibility: MCP tools focus on user functionality, analytics collection is secondary
- Open/Closed: Tools extended with analytics without modifying core search logic
- Dependency Inversion: Tools depend on analytics service abstraction through container DI

**Existing Integration Points**:
- **Story 2.1**: Uses analytics service infrastructure for data collection and storage
- **Container DI**: Leverages existing dependency injection for analytics service access
- **MCP Server**: Extends existing FastMCP server with enhanced tools
- **RagAPI**: Uses existing search functionality without modification

## Testing

**Test File Location**: `mcp_rag_playground/tests/` following existing pattern

**Test Standards**:
- Use pytest framework with async testing capabilities
- Follow existing marker system (unit, integration, performance)
- Achieve 80% minimum code coverage for new MCP tool functionality
- Test both analytics-enabled and analytics-disabled scenarios
- Mock analytics service for unit tests

**Testing Frameworks and Patterns**:
- Pytest with pytest-asyncio for async MCP tool testing
- MCP tool testing patterns for FastMCP framework
- Performance benchmarking for response time validation
- Mock objects for analytics service isolation

**Specific Testing Requirements**:
- Enhanced search tool functionality: Test search results with analytics collection
- User feedback system: Test rating validation and feedback processing
- Async analytics collection: Test non-blocking behavior and error handling
- Query ID linking: Test UUID generation and feedback association
- Performance isolation: Benchmark tool response times with analytics enabled/disabled
- Backward compatibility: Test tool behavior matches existing functionality

## Change Log
| Date | Version | Description | Author |
|------|---------|-------------|-----------|
| 2025-08-16 | 1.0 | Initial story creation for MCP tool analytics integration | PM John |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used

### Debug Log References

### Completion Notes List

### File List

## QA Results
*This section will be populated by the QA agent after story completion*